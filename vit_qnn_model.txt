QuantModel(
  (model): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adaptive_padding): AdaptivePadding()
      (projection): QuantModule(
        (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
        (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
        (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
        (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
        (activation_function): StraightThrough()
      )
    )
    (drop_after_pos): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (qkv): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.1, inplace=False)
          (out_drop): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): QuantModule(
                (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
                (activation_function): StraightThrough()
              )
              (1): GELU(approximate=none)
              (2): Dropout(p=0.1, inplace=False)
            )
            (1): QuantModule(
              (weight_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (bias_quantizer): RistrettoQuantizer(bit=4, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (in_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (out_act_quantizer): RistrettoQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=False)
              (activation_function): StraightThrough()
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d', 'mode': 'fan_in', 'nonlinearity': 'linear'}]
)