VisionTransformer(
  (patch_embed): PatchEmbed(
    (adaptive_padding): AdaptivePadding()
    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (drop_after_pos): Dropout(p=0.1, inplace=False)
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (1): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (2): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (3): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (4): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (5): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (6): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (7): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (8): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (9): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (10): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
    (11): TransformerEncoderLayer(
      (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): MultiheadAttention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
        (out_drop): DropPath()
      )
      (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (ffn): FFN(
        (activate): GELU(approximate=none)
        (layers): Sequential(
          (0): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate=none)
            (2): Dropout(p=0.1, inplace=False)
          )
          (1): Linear(in_features=3072, out_features=768, bias=True)
          (2): Dropout(p=0.1, inplace=False)
        )
        (dropout_layer): DropPath()
      )
    )
  )
  (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d', 'mode': 'fan_in', 'nonlinearity': 'linear'}]